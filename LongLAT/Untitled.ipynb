{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ce4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (0.15.12)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (1.33.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (59.5.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (4.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.5)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.9.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\mkanneganti\\anaconda3\\lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7eff159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>CombinedChunk</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137090</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166588</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[0, 0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195209</td>\n",
       "      <td>date of birth:          sex:  m service csu ad...</td>\n",
       "      <td>[0, 1, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133416</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173812</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6175</th>\n",
       "      <td>109483</td>\n",
       "      <td>date of birth:          sex:  m service illnes...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176</th>\n",
       "      <td>109365</td>\n",
       "      <td>date of birth:                    sex: service...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6177</th>\n",
       "      <td>134157</td>\n",
       "      <td>date of birth:        sex:  f servicehospital ...</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>183373</td>\n",
       "      <td>service neurology history this is a year old r...</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6179</th>\n",
       "      <td>183363</td>\n",
       "      <td>date of birth:                    sex: service...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6180 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id                                      CombinedChunk  \\\n",
       "0      137090  date of birth:               sex:   m service ...   \n",
       "1      166588  date of birth:               sex:   f service ...   \n",
       "2      195209  date of birth:          sex:  m service csu ad...   \n",
       "3      133416  date of birth:               sex:   m service ...   \n",
       "4      173812  date of birth:               sex:   m service ...   \n",
       "...       ...                                                ...   \n",
       "6175   109483  date of birth:          sex:  m service illnes...   \n",
       "6176   109365  date of birth:                    sex: service...   \n",
       "6177   134157  date of birth:        sex:  f servicehospital ...   \n",
       "6178   183373  service neurology history this is a year old r...   \n",
       "6179   183363  date of birth:                    sex: service...   \n",
       "\n",
       "               labels  \n",
       "0     [1, 1, 0, 0, 0]  \n",
       "1     [0, 0, 0, 1, 1]  \n",
       "2     [0, 1, 1, 0, 1]  \n",
       "3     [1, 0, 0, 0, 0]  \n",
       "4     [1, 0, 0, 0, 0]  \n",
       "...               ...  \n",
       "6175  [1, 0, 0, 0, 0]  \n",
       "6176  [0, 1, 0, 0, 0]  \n",
       "6177  [1, 1, 0, 0, 0]  \n",
       "6178  [0, 1, 1, 1, 0]  \n",
       "6179  [0, 1, 0, 0, 0]  \n",
       "\n",
       "[6180 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import LongformerTokenizerFast, LongformerModel, LongformerConfig, Trainer, TrainingArguments, EvalPrediction, AutoTokenizer, AutoModel\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel, LongformerClassificationHead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# read the dataframe\n",
    "train = pd.read_csv('../data/mimic3/5/train_data_5_level_1.csv')\n",
    "train['Chunk1'] = train['Chunk1'].fillna('')\n",
    "train['Chunk2'] = train['Chunk2'].fillna('')\n",
    "train['Chunk3'] = train['Chunk3'].fillna('')\n",
    "train['Chunk4'] = train['Chunk4'].fillna('')\n",
    "train['Chunk5'] = train['Chunk5'].fillna('')\n",
    "train['Chunk6'] = train['Chunk6'].fillna('')\n",
    "train['Chunk7'] = train['Chunk7'].fillna('')\n",
    "train['Chunk8'] = train['Chunk8'].fillna('')\n",
    "train['Chunk9'] = train['Chunk9'].fillna('')\n",
    "train['Chunk10'] = train['Chunk10'].fillna('')\n",
    "\n",
    "# Concatenate the 'Chunk' columns into a single 'CombinedChunk' column\n",
    "train['CombinedChunk'] = train['Chunk1'] + train['Chunk2'] + train['Chunk3'] + train['Chunk4'] + train['Chunk5'] + train['Chunk6'] + train['Chunk7'] + train['Chunk8'] + train['Chunk9'] + train['Chunk10']\n",
    "train = train.drop(['Chunk1', 'Chunk2', 'Chunk3', 'Chunk4', 'Chunk5', 'Chunk6', 'Chunk7', 'Chunk8', 'Chunk9', 'Chunk10'], axis=1)\n",
    "train\n",
    "column_order = ['hadm_id', 'CombinedChunk', '38.93', '401.9', '414.01', '427.31', '428.0']\n",
    "\n",
    "# Reorder the columns in the DataFrame based on the specified order\n",
    "train = train[column_order]\n",
    "train['labels'] = train[train.columns[2:]].values.tolist()\n",
    "train = train.drop(['38.93', '401.9', '414.01', '427.31', '428.0'], axis=1)\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c614e078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>CombinedChunk</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142423</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>187232</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199859</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[0, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135343</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191783</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>103310</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>101071</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[0, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>124309</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>129034</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>158927</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1358 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id                                      CombinedChunk  \\\n",
       "0      142423  date of birth:               sex:   m service ...   \n",
       "1      187232  date of birth:               sex:   f service ...   \n",
       "2      199859  date of birth:               sex:   f service ...   \n",
       "3      135343  date of birth:               sex:   m service ...   \n",
       "4      191783  date of birth:               sex:   m service ...   \n",
       "...       ...                                                ...   \n",
       "1353   103310  date of birth:               sex:   m service ...   \n",
       "1354   101071  date of birth:               sex:   f service ...   \n",
       "1355   124309  date of birth:               sex:   f service ...   \n",
       "1356   129034  date of birth:               sex:   m service ...   \n",
       "1357   158927  date of birth:               sex:   f service ...   \n",
       "\n",
       "               labels  \n",
       "0     [1, 0, 0, 0, 0]  \n",
       "1     [0, 1, 0, 0, 0]  \n",
       "2     [0, 0, 1, 1, 1]  \n",
       "3     [0, 1, 0, 0, 0]  \n",
       "4     [1, 0, 0, 0, 0]  \n",
       "...               ...  \n",
       "1353  [0, 1, 0, 0, 0]  \n",
       "1354  [0, 1, 1, 0, 0]  \n",
       "1355  [1, 0, 0, 0, 0]  \n",
       "1356  [0, 0, 0, 1, 0]  \n",
       "1357  [0, 1, 0, 0, 0]  \n",
       "\n",
       "[1358 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the 'test' DataFrame from a CSV file (replace '/path/to/test_data.csv' with the actual path)\n",
    "test = pd.read_csv('../data/mimic3/5/test_data_5_level_1.csv')\n",
    "\n",
    "# Fill NaN values in 'Chunk' columns with empty strings\n",
    "test['Chunk1'] = test['Chunk1'].fillna('')\n",
    "test['Chunk2'] = test['Chunk2'].fillna('')\n",
    "test['Chunk3'] = test['Chunk3'].fillna('')\n",
    "test['Chunk4'] = test['Chunk4'].fillna('')\n",
    "test['Chunk5'] = test['Chunk5'].fillna('')\n",
    "test['Chunk6'] = test['Chunk6'].fillna('')\n",
    "test['Chunk7'] = test['Chunk7'].fillna('')\n",
    "test['Chunk8'] = test['Chunk8'].fillna('')\n",
    "test['Chunk9'] = test['Chunk9'].fillna('')\n",
    "test['Chunk10'] = test['Chunk10'].fillna('')\n",
    "\n",
    "# Concatenate the 'Chunk' columns into a single 'CombinedChunk' column\n",
    "test['CombinedChunk'] = test['Chunk1'] + test['Chunk2'] + test['Chunk3'] + test['Chunk4'] + test['Chunk5'] + test['Chunk6'] + test['Chunk7'] + test['Chunk8'] + test['Chunk9'] + test['Chunk10']\n",
    "test = test.drop(['Chunk1', 'Chunk2', 'Chunk3', 'Chunk4', 'Chunk5', 'Chunk6', 'Chunk7', 'Chunk8', 'Chunk9', 'Chunk10'], axis=1)\n",
    "\n",
    "# Define the desired column order for 'test' (similar to 'train')\n",
    "column_order = ['hadm_id', 'CombinedChunk', '38.93', '401.9', '414.01', '427.31', '428.0']\n",
    "\n",
    "# Reorder the columns in the 'test' DataFrame based on the specified order\n",
    "test = test[column_order]\n",
    "\n",
    "# Create a 'labels' column in 'test' (if needed)\n",
    "test['labels'] = test[test.columns[2:]].values.tolist()\n",
    "test = test.drop(['38.93', '401.9', '414.01', '427.31', '428.0'], axis=1)\n",
    "test\n",
    "# Now, you have performed the same operations on the 'test' DataFrame as you did for 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f00e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>CombinedChunk</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143537</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167925</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150956</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[1, 0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180164</td>\n",
       "      <td>service medicine allergies patient recorded as...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180767</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>188594</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>151432</td>\n",
       "      <td>service neurosurgery allergies patient recorde...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>152868</td>\n",
       "      <td>date of birth:               sex:   m service ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>180431</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>111912</td>\n",
       "      <td>date of birth:               sex:   f service ...</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1199 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id                                      CombinedChunk  \\\n",
       "0      143537  date of birth:               sex:   m service ...   \n",
       "1      167925  date of birth:               sex:   m service ...   \n",
       "2      150956  date of birth:               sex:   m service ...   \n",
       "3      180164  service medicine allergies patient recorded as...   \n",
       "4      180767  date of birth:               sex:   f service ...   \n",
       "...       ...                                                ...   \n",
       "1194   188594  date of birth:               sex:   m service ...   \n",
       "1195   151432  service neurosurgery allergies patient recorde...   \n",
       "1196   152868  date of birth:               sex:   m service ...   \n",
       "1197   180431  date of birth:               sex:   f service ...   \n",
       "1198   111912  date of birth:               sex:   f service ...   \n",
       "\n",
       "               labels  \n",
       "0     [1, 0, 0, 0, 0]  \n",
       "1     [0, 1, 0, 0, 0]  \n",
       "2     [1, 0, 0, 1, 1]  \n",
       "3     [0, 1, 0, 0, 0]  \n",
       "4     [0, 1, 1, 1, 0]  \n",
       "...               ...  \n",
       "1194  [0, 1, 0, 0, 0]  \n",
       "1195  [0, 0, 0, 1, 0]  \n",
       "1196  [0, 1, 0, 0, 0]  \n",
       "1197  [1, 0, 0, 0, 0]  \n",
       "1198  [0, 1, 1, 1, 0]  \n",
       "\n",
       "[1199 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the 'dev' DataFrame from a CSV file (replace '/path/to/dev_data.csv' with the actual path)\n",
    "dev = pd.read_csv('../data/mimic3/5/dev_data_5_level_1.csv')\n",
    "\n",
    "# Fill NaN values in 'Chunk' columns with empty strings\n",
    "dev['Chunk1'] = dev['Chunk1'].fillna('')\n",
    "dev['Chunk2'] = dev['Chunk2'].fillna('')\n",
    "dev['Chunk3'] = dev['Chunk3'].fillna('')\n",
    "dev['Chunk4'] = dev['Chunk4'].fillna('')\n",
    "dev['Chunk5'] = dev['Chunk5'].fillna('')\n",
    "dev['Chunk6'] = dev['Chunk6'].fillna('')\n",
    "dev['Chunk7'] = dev['Chunk7'].fillna('')\n",
    "dev['Chunk8'] = dev['Chunk8'].fillna('')\n",
    "dev['Chunk9'] = dev['Chunk9'].fillna('')\n",
    "dev['Chunk10'] = dev['Chunk10'].fillna('')\n",
    "\n",
    "# Concatenate the 'Chunk' columns into a single 'CombinedChunk' column\n",
    "dev['CombinedChunk'] = dev['Chunk1'] + dev['Chunk2'] + dev['Chunk3'] + dev['Chunk4'] + dev['Chunk5'] + dev['Chunk6'] + dev['Chunk7'] + dev['Chunk8'] + dev['Chunk9'] + dev['Chunk10']\n",
    "dev = dev.drop(['Chunk1', 'Chunk2', 'Chunk3', 'Chunk4', 'Chunk5', 'Chunk6', 'Chunk7', 'Chunk8', 'Chunk9', 'Chunk10'], axis=1)\n",
    "\n",
    "# Define the desired column order for 'dev' (similar to 'train' and 'test')\n",
    "column_order = ['hadm_id', 'CombinedChunk', '38.93', '401.9', '414.01', '427.31', '428.0']\n",
    "\n",
    "# Reorder the columns in the 'dev' DataFrame based on the specified order\n",
    "dev = dev[column_order]\n",
    "\n",
    "# Create a 'labels' column in 'dev' (if needed)\n",
    "dev['labels'] = dev[dev.columns[2:]].values.tolist()\n",
    "dev = dev.drop(['38.93', '401.9', '414.01', '427.31', '428.0'], axis=1)\n",
    "dev\n",
    "# Now, you have performed the same operations on the 'dev' DataFrame as you did for 'train' and 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8735a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongformerForMultiLabelICDClassification(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    We instantiate a class of LongFormer adapted for a multilabel classification task.\n",
    "    This instance takes the pooled output of the LongFormer based model and passes it through a classification head. We replace the traditional Cross Entropy loss with a BCE loss that generate probabilities for all the labels that we feed into the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LongformerForMultiLabelICDClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.transformer_layer = AutoModel.from_pretrained(\"pretrained/ClinicalplusXLNet/\")\n",
    "        self.longformer = LongformerModel(config)\n",
    "        self.classifier = LongformerClassificationHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None,\n",
    "                token_type_ids=None, position_ids=None, inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        # create global attention on sequence, and a global attention token on the `s` token\n",
    "        # the equivalent of the CLS token on BERT models. This is taken care of by HuggingFace\n",
    "        # on the LongformerForSequenceClassification class\n",
    "        if global_attention_mask is None:\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "        #transformer_output = self.transformer_layer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        # pass arguments to longformer model\n",
    "        #transformer_output = tuple(torch.tensor(tf.convert_to_tensor(hs)).detach().numpy() for hs in transformer_output.hidden_states)\n",
    "        outputs = self.longformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            global_attention_mask = global_attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids)\n",
    "\n",
    "        # if specified the model can return a dict where each key corresponds to the output of a\n",
    "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
    "        # which will have the shape (batch_size, sequence_length, hidden_size).\n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "\n",
    "        # pass the hidden states through the classifier to obtain thee logits\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            labels = labels.float()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                            labels.view(-1, self.num_labels))\n",
    "            #outputs = (loss,) + outputs\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274b9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Processing(object):\n",
    "    def __init__(self, tokenizer, id_column, text_column, label_column):\n",
    "\n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "\n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column\n",
    "\n",
    "        # define the id column and transform it to list\n",
    "        self.id_column = id_column.tolist()\n",
    "\n",
    "\n",
    "# iter method to get each element at the time and tokenize it using bert\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.text_column[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "        # encode the sequence and add padding\n",
    "        inputs = tokenizer.encode_plus(comment_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length= 3048,\n",
    "                                       padding = 'max_length',\n",
    "                                       return_attention_mask = True,\n",
    "                                       truncation = True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        labels_ = torch.tensor(self.label_column[index], dtype=torch.float)\n",
    "        id_ = self.id_column[index]\n",
    "        return {'input_ids':input_ids[0], 'attention_mask':attention_mask[0],\n",
    "                'labels':labels_, 'id_':id_}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_column)\n",
    "\n",
    "batch_size = 2\n",
    "# create a class to process the training and test data\n",
    "tokenizer = AutoTokenizer.from_pretrained('yikuan8/Clinical-Longformer',\n",
    "                                                    padding = 'max_length',\n",
    "                                                    truncation=False,\n",
    "                                                    max_length = 4096,\n",
    "                                                    padding_side=\"right\")\n",
    "training_data = Data_Processing(tokenizer,\n",
    "                                train['hadm_id'],\n",
    "                                train['CombinedChunk'],\n",
    "                                train['labels'])\n",
    "dev_data = Data_Processing(tokenizer,\n",
    "                             dev['hadm_id'],\n",
    "                             dev['CombinedChunk'],\n",
    "                             dev['labels'])\n",
    "test_data =  Data_Processing(tokenizer,\n",
    "                             test['hadm_id'],\n",
    "                             test['CombinedChunk'],\n",
    "                             test['labels'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "                    'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                 'val':len(test_data)\n",
    "                }\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eba6938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForMultiLabelICDClassification were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['transformer_layer.layer.11.ff.layer_2.weight', 'transformer_layer.layer.8.ff.layer_2.weight', 'transformer_layer.layer.4.ff.layer_1.bias', 'transformer_layer.layer.6.ff.layer_norm.bias', 'transformer_layer.layer.3.ff.layer_2.bias', 'transformer_layer.layer.9.rel_attn.o', 'transformer_layer.layer.9.ff.layer_2.bias', 'transformer_layer.layer.8.rel_attn.layer_norm.weight', 'transformer_layer.layer.7.rel_attn.layer_norm.weight', 'transformer_layer.layer.10.rel_attn.q', 'transformer_layer.layer.10.rel_attn.r', 'transformer_layer.layer.3.rel_attn.r_r_bias', 'transformer_layer.layer.10.rel_attn.v', 'transformer_layer.layer.4.rel_attn.r', 'transformer_layer.layer.5.rel_attn.r_r_bias', 'transformer_layer.layer.7.rel_attn.q', 'transformer_layer.layer.7.ff.layer_norm.weight', 'transformer_layer.layer.6.ff.layer_1.weight', 'transformer_layer.layer.10.rel_attn.r_s_bias', 'transformer_layer.layer.11.ff.layer_norm.bias', 'transformer_layer.layer.10.ff.layer_2.bias', 'transformer_layer.layer.2.rel_attn.k', 'transformer_layer.layer.1.rel_attn.k', 'transformer_layer.layer.2.ff.layer_1.bias', 'transformer_layer.layer.1.ff.layer_2.weight', 'transformer_layer.layer.2.ff.layer_2.weight', 'transformer_layer.layer.0.rel_attn.layer_norm.bias', 'transformer_layer.layer.8.rel_attn.seg_embed', 'transformer_layer.layer.6.rel_attn.layer_norm.bias', 'transformer_layer.layer.4.rel_attn.o', 'transformer_layer.layer.5.rel_attn.layer_norm.weight', 'transformer_layer.layer.10.ff.layer_norm.bias', 'transformer_layer.layer.9.rel_attn.r_r_bias', 'transformer_layer.layer.0.rel_attn.r_r_bias', 'transformer_layer.layer.5.ff.layer_1.weight', 'transformer_layer.layer.5.rel_attn.k', 'transformer_layer.layer.7.rel_attn.r', 'transformer_layer.layer.7.rel_attn.k', 'transformer_layer.layer.7.rel_attn.v', 'transformer_layer.layer.0.ff.layer_2.bias', 'transformer_layer.layer.8.rel_attn.r_r_bias', 'transformer_layer.layer.11.rel_attn.r_s_bias', 'transformer_layer.layer.1.rel_attn.layer_norm.bias', 'transformer_layer.layer.11.rel_attn.layer_norm.bias', 'transformer_layer.layer.3.ff.layer_norm.weight', 'transformer_layer.layer.7.rel_attn.r_w_bias', 'transformer_layer.layer.9.ff.layer_1.bias', 'transformer_layer.layer.6.rel_attn.q', 'transformer_layer.layer.8.rel_attn.layer_norm.bias', 'transformer_layer.layer.10.ff.layer_1.bias', 'transformer_layer.layer.9.rel_attn.q', 'transformer_layer.layer.8.rel_attn.o', 'transformer_layer.layer.6.ff.layer_2.bias', 'transformer_layer.layer.9.ff.layer_norm.bias', 'transformer_layer.layer.5.rel_attn.r_s_bias', 'transformer_layer.layer.4.rel_attn.q', 'transformer_layer.layer.3.rel_attn.q', 'transformer_layer.layer.4.rel_attn.seg_embed', 'transformer_layer.layer.4.rel_attn.k', 'transformer_layer.layer.8.ff.layer_1.weight', 'transformer_layer.layer.10.rel_attn.layer_norm.bias', 'transformer_layer.layer.5.rel_attn.layer_norm.bias', 'transformer_layer.layer.11.rel_attn.q', 'transformer_layer.layer.2.ff.layer_2.bias', 'transformer_layer.layer.8.rel_attn.r_s_bias', 'longformer.pooler.dense.weight', 'transformer_layer.layer.3.ff.layer_norm.bias', 'transformer_layer.layer.8.ff.layer_2.bias', 'transformer_layer.layer.1.ff.layer_norm.weight', 'transformer_layer.layer.5.ff.layer_2.weight', 'transformer_layer.layer.11.ff.layer_norm.weight', 'transformer_layer.layer.1.rel_attn.r_s_bias', 'transformer_layer.layer.1.rel_attn.r_w_bias', 'transformer_layer.layer.4.rel_attn.layer_norm.weight', 'transformer_layer.layer.6.rel_attn.v', 'transformer_layer.layer.4.rel_attn.r_w_bias', 'transformer_layer.layer.4.ff.layer_norm.bias', 'transformer_layer.layer.3.rel_attn.k', 'transformer_layer.layer.6.ff.layer_2.weight', 'transformer_layer.layer.10.rel_attn.r_r_bias', 'transformer_layer.layer.4.ff.layer_norm.weight', 'transformer_layer.layer.4.rel_attn.r_r_bias', 'transformer_layer.layer.1.ff.layer_1.weight', 'transformer_layer.layer.2.rel_attn.seg_embed', 'transformer_layer.layer.9.rel_attn.v', 'transformer_layer.layer.10.ff.layer_2.weight', 'transformer_layer.layer.2.rel_attn.layer_norm.bias', 'transformer_layer.layer.9.rel_attn.k', 'transformer_layer.layer.2.rel_attn.r_r_bias', 'transformer_layer.layer.1.rel_attn.o', 'transformer_layer.layer.7.rel_attn.r_s_bias', 'transformer_layer.layer.3.rel_attn.v', 'transformer_layer.layer.6.rel_attn.r_s_bias', 'transformer_layer.layer.9.ff.layer_norm.weight', 'transformer_layer.layer.2.ff.layer_1.weight', 'transformer_layer.layer.3.rel_attn.seg_embed', 'transformer_layer.layer.0.ff.layer_norm.bias', 'transformer_layer.layer.9.ff.layer_1.weight', 'transformer_layer.layer.6.ff.layer_1.bias', 'transformer_layer.layer.8.rel_attn.v', 'transformer_layer.layer.0.rel_attn.layer_norm.weight', 'transformer_layer.layer.0.rel_attn.o', 'transformer_layer.layer.3.rel_attn.layer_norm.bias', 'transformer_layer.layer.5.rel_attn.o', 'transformer_layer.layer.3.ff.layer_1.weight', 'transformer_layer.layer.11.ff.layer_1.bias', 'transformer_layer.layer.8.rel_attn.r', 'transformer_layer.layer.3.ff.layer_1.bias', 'transformer_layer.layer.9.rel_attn.r', 'transformer_layer.layer.5.rel_attn.seg_embed', 'transformer_layer.layer.5.ff.layer_norm.bias', 'transformer_layer.layer.0.ff.layer_1.bias', 'transformer_layer.layer.2.rel_attn.q', 'transformer_layer.layer.5.rel_attn.v', 'transformer_layer.layer.6.rel_attn.o', 'transformer_layer.layer.10.rel_attn.o', 'transformer_layer.layer.9.rel_attn.layer_norm.bias', 'transformer_layer.layer.7.rel_attn.o', 'classifier.out_proj.bias', 'transformer_layer.layer.11.rel_attn.v', 'transformer_layer.layer.7.ff.layer_norm.bias', 'transformer_layer.layer.0.rel_attn.r_s_bias', 'transformer_layer.word_embedding.weight', 'transformer_layer.layer.2.rel_attn.r', 'transformer_layer.layer.1.rel_attn.r', 'transformer_layer.layer.7.rel_attn.layer_norm.bias', 'transformer_layer.layer.0.rel_attn.seg_embed', 'transformer_layer.layer.1.ff.layer_2.bias', 'transformer_layer.layer.5.rel_attn.q', 'transformer_layer.layer.9.rel_attn.seg_embed', 'transformer_layer.layer.0.rel_attn.v', 'transformer_layer.layer.4.rel_attn.v', 'transformer_layer.layer.3.rel_attn.r_s_bias', 'longformer.pooler.dense.bias', 'transformer_layer.layer.0.rel_attn.q', 'transformer_layer.layer.6.rel_attn.r_r_bias', 'transformer_layer.layer.5.rel_attn.r', 'transformer_layer.layer.4.rel_attn.layer_norm.bias', 'transformer_layer.layer.2.rel_attn.o', 'transformer_layer.layer.8.rel_attn.r_w_bias', 'transformer_layer.layer.1.rel_attn.r_r_bias', 'transformer_layer.layer.6.rel_attn.seg_embed', 'transformer_layer.layer.6.ff.layer_norm.weight', 'transformer_layer.layer.2.rel_attn.v', 'transformer_layer.layer.1.ff.layer_1.bias', 'transformer_layer.layer.1.rel_attn.q', 'classifier.dense.bias', 'transformer_layer.layer.3.rel_attn.o', 'transformer_layer.layer.3.rel_attn.r_w_bias', 'transformer_layer.layer.6.rel_attn.layer_norm.weight', 'transformer_layer.layer.11.rel_attn.r', 'transformer_layer.layer.4.ff.layer_2.bias', 'transformer_layer.layer.11.rel_attn.layer_norm.weight', 'transformer_layer.layer.11.rel_attn.seg_embed', 'transformer_layer.layer.10.rel_attn.k', 'transformer_layer.layer.5.ff.layer_1.bias', 'transformer_layer.layer.9.rel_attn.layer_norm.weight', 'transformer_layer.layer.7.ff.layer_1.bias', 'transformer_layer.layer.6.rel_attn.r', 'transformer_layer.layer.4.ff.layer_1.weight', 'transformer_layer.layer.8.rel_attn.k', 'transformer_layer.layer.2.ff.layer_norm.weight', 'transformer_layer.layer.0.ff.layer_norm.weight', 'transformer_layer.layer.3.rel_attn.r', 'transformer_layer.layer.2.ff.layer_norm.bias', 'transformer_layer.layer.6.rel_attn.k', 'transformer_layer.layer.7.rel_attn.seg_embed', 'transformer_layer.layer.1.rel_attn.seg_embed', 'transformer_layer.layer.11.rel_attn.k', 'transformer_layer.layer.10.rel_attn.layer_norm.weight', 'transformer_layer.layer.7.ff.layer_2.weight', 'transformer_layer.layer.7.ff.layer_2.bias', 'transformer_layer.layer.11.rel_attn.r_w_bias', 'transformer_layer.layer.5.rel_attn.r_w_bias', 'transformer_layer.layer.0.rel_attn.r', 'transformer_layer.layer.0.rel_attn.r_w_bias', 'transformer_layer.layer.4.rel_attn.r_s_bias', 'transformer_layer.layer.3.ff.layer_2.weight', 'transformer_layer.layer.4.ff.layer_2.weight', 'classifier.dense.weight', 'transformer_layer.layer.2.rel_attn.r_w_bias', 'transformer_layer.layer.10.ff.layer_norm.weight', 'transformer_layer.layer.8.ff.layer_norm.weight', 'transformer_layer.layer.9.ff.layer_2.weight', 'transformer_layer.layer.8.ff.layer_1.bias', 'transformer_layer.layer.1.rel_attn.layer_norm.weight', 'transformer_layer.layer.7.rel_attn.r_r_bias', 'transformer_layer.layer.10.ff.layer_1.weight', 'transformer_layer.layer.1.ff.layer_norm.bias', 'transformer_layer.layer.11.ff.layer_2.bias', 'transformer_layer.layer.0.ff.layer_2.weight', 'transformer_layer.layer.11.rel_attn.o', 'transformer_layer.layer.0.ff.layer_1.weight', 'transformer_layer.layer.2.rel_attn.layer_norm.weight', 'transformer_layer.mask_emb', 'transformer_layer.layer.2.rel_attn.r_s_bias', 'transformer_layer.layer.9.rel_attn.r_s_bias', 'transformer_layer.layer.11.rel_attn.r_r_bias', 'transformer_layer.layer.9.rel_attn.r_w_bias', 'transformer_layer.layer.5.ff.layer_2.bias', 'transformer_layer.layer.5.ff.layer_norm.weight', 'transformer_layer.layer.3.rel_attn.layer_norm.weight', 'transformer_layer.layer.8.rel_attn.q', 'transformer_layer.layer.6.rel_attn.r_w_bias', 'transformer_layer.layer.11.ff.layer_1.weight', 'transformer_layer.layer.10.rel_attn.seg_embed', 'transformer_layer.layer.10.rel_attn.r_w_bias', 'transformer_layer.layer.8.ff.layer_norm.bias', 'transformer_layer.layer.0.rel_attn.k', 'transformer_layer.layer.1.rel_attn.v', 'transformer_layer.layer.7.ff.layer_1.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LongformerForMultiLabelICDClassification.from_pretrained('yikuan8/Clinical-Longformer',\n",
    "                                                  gradient_checkpointing=False,\n",
    "                                                  attention_window = 512,\n",
    "                                                  num_labels = 5,\n",
    "                                                  return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f0c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmeghanaraokanneganti\u001b[0m (\u001b[33mmeghanarao\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\mkanneganti\\Downloads\\ICD_Coding\\HiLAT-main\\HiLAT-main\\wandb\\run-20231101_172929-ogso188m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/meghanarao/huggingface/runs/ogso188m' target=\"_blank\">longformer_multilabel_paper_trainer_3048_2e5</a></strong> to <a href='https://wandb.ai/meghanarao/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/meghanarao/huggingface' target=\"_blank\">https://wandb.ai/meghanarao/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/meghanarao/huggingface/runs/ogso188m' target=\"_blank\">https://wandb.ai/meghanarao/huggingface/runs/ogso188m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "def multi_label_metrics(predictions, labels):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_true = labels\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # define dictionary of metrics to return\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "# Use the aux EvalPrediction class to obtain prediction labels\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions,\n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds,\n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '../model/new',\n",
    "    num_train_epochs = 10,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 64,\n",
    "    per_device_eval_batch_size= 2,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    disable_tqdm = False,\n",
    "    load_best_model_at_end=False,\n",
    "    warmup_steps = 2000,\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    fp16 = False,\n",
    "    logging_dir='/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'longformer_multilabel_paper_trainer_3048_2e5'\n",
    ")\n",
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #data_collator = Data_Processing(),\n",
    "\n",
    ")\n",
    "device = 'cpu'\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24599b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers --upgrade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
