{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcef8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForMultiLabelICDClassification were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['transformer_layer.layer.3.rel_attn.o', 'transformer_layer.layer.4.rel_attn.r', 'transformer_layer.layer.2.ff.layer_2.bias', 'transformer_layer.layer.6.rel_attn.r_s_bias', 'transformer_layer.layer.5.rel_attn.v', 'transformer_layer.layer.4.rel_attn.r_s_bias', 'transformer_layer.layer.6.ff.layer_1.weight', 'transformer_layer.layer.11.rel_attn.r_r_bias', 'transformer_layer.layer.7.ff.layer_1.bias', 'transformer_layer.layer.10.ff.layer_norm.bias', 'transformer_layer.layer.11.ff.layer_2.weight', 'transformer_layer.layer.10.ff.layer_norm.weight', 'transformer_layer.layer.4.rel_attn.k', 'transformer_layer.layer.9.rel_attn.r_s_bias', 'transformer_layer.layer.0.ff.layer_2.weight', 'transformer_layer.layer.3.rel_attn.q', 'transformer_layer.layer.9.rel_attn.r_r_bias', 'transformer_layer.layer.5.ff.layer_1.bias', 'transformer_layer.layer.5.rel_attn.r_s_bias', 'transformer_layer.layer.10.rel_attn.r_w_bias', 'transformer_layer.layer.9.rel_attn.layer_norm.weight', 'transformer_layer.layer.11.rel_attn.o', 'transformer_layer.layer.11.ff.layer_1.bias', 'transformer_layer.layer.4.rel_attn.o', 'transformer_layer.layer.5.rel_attn.o', 'transformer_layer.layer.5.ff.layer_norm.weight', 'transformer_layer.layer.4.ff.layer_1.weight', 'transformer_layer.layer.6.rel_attn.seg_embed', 'transformer_layer.layer.1.ff.layer_norm.bias', 'transformer_layer.layer.10.rel_attn.v', 'transformer_layer.layer.8.ff.layer_2.weight', 'transformer_layer.layer.1.ff.layer_norm.weight', 'transformer_layer.layer.3.ff.layer_2.weight', 'transformer_layer.layer.0.rel_attn.r_w_bias', 'transformer_layer.layer.5.rel_attn.r', 'transformer_layer.layer.10.rel_attn.o', 'transformer_layer.layer.10.ff.layer_1.weight', 'transformer_layer.layer.6.ff.layer_norm.bias', 'transformer_layer.layer.9.ff.layer_2.bias', 'transformer_layer.layer.3.rel_attn.layer_norm.weight', 'transformer_layer.layer.1.rel_attn.r_r_bias', 'transformer_layer.layer.7.rel_attn.q', 'transformer_layer.layer.5.rel_attn.seg_embed', 'transformer_layer.layer.7.ff.layer_norm.bias', 'classifier.out_proj.weight', 'transformer_layer.layer.0.ff.layer_2.bias', 'transformer_layer.layer.6.ff.layer_2.weight', 'transformer_layer.layer.10.ff.layer_2.weight', 'transformer_layer.layer.3.rel_attn.r_r_bias', 'transformer_layer.layer.8.rel_attn.layer_norm.bias', 'transformer_layer.layer.9.ff.layer_1.weight', 'transformer_layer.layer.0.rel_attn.k', 'transformer_layer.layer.5.rel_attn.q', 'transformer_layer.layer.3.rel_attn.r', 'transformer_layer.layer.3.rel_attn.r_s_bias', 'transformer_layer.layer.10.rel_attn.q', 'transformer_layer.layer.0.rel_attn.seg_embed', 'transformer_layer.layer.11.ff.layer_1.weight', 'transformer_layer.layer.4.ff.layer_2.weight', 'transformer_layer.layer.10.rel_attn.r_s_bias', 'transformer_layer.layer.2.rel_attn.k', 'transformer_layer.layer.2.rel_attn.v', 'transformer_layer.word_embedding.weight', 'transformer_layer.layer.6.rel_attn.r_w_bias', 'transformer_layer.layer.2.rel_attn.r_w_bias', 'transformer_layer.layer.1.rel_attn.layer_norm.bias', 'transformer_layer.layer.7.rel_attn.r_s_bias', 'transformer_layer.layer.7.ff.layer_1.weight', 'transformer_layer.layer.11.rel_attn.r_w_bias', 'transformer_layer.layer.1.rel_attn.q', 'transformer_layer.layer.1.ff.layer_2.weight', 'transformer_layer.layer.1.ff.layer_1.weight', 'transformer_layer.layer.0.ff.layer_1.weight', 'transformer_layer.layer.2.rel_attn.layer_norm.weight', 'transformer_layer.layer.3.ff.layer_norm.weight', 'transformer_layer.layer.6.ff.layer_norm.weight', 'transformer_layer.layer.11.ff.layer_2.bias', 'transformer_layer.layer.3.rel_attn.r_w_bias', 'transformer_layer.layer.9.ff.layer_1.bias', 'transformer_layer.layer.4.rel_attn.v', 'transformer_layer.layer.2.ff.layer_2.weight', 'transformer_layer.layer.4.ff.layer_norm.bias', 'transformer_layer.layer.2.rel_attn.seg_embed', 'transformer_layer.layer.9.ff.layer_norm.weight', 'transformer_layer.layer.2.rel_attn.o', 'transformer_layer.layer.6.rel_attn.q', 'transformer_layer.layer.6.rel_attn.v', 'transformer_layer.layer.11.rel_attn.k', 'transformer_layer.layer.9.ff.layer_2.weight', 'transformer_layer.layer.7.ff.layer_2.weight', 'transformer_layer.layer.0.ff.layer_norm.weight', 'transformer_layer.layer.1.rel_attn.v', 'transformer_layer.layer.5.ff.layer_norm.bias', 'transformer_layer.layer.7.rel_attn.layer_norm.bias', 'transformer_layer.layer.11.rel_attn.r', 'transformer_layer.layer.3.rel_attn.layer_norm.bias', 'transformer_layer.layer.8.rel_attn.k', 'transformer_layer.layer.8.ff.layer_norm.weight', 'transformer_layer.layer.10.rel_attn.layer_norm.weight', 'transformer_layer.layer.7.rel_attn.r_r_bias', 'classifier.dense.bias', 'transformer_layer.layer.4.ff.layer_1.bias', 'transformer_layer.layer.0.rel_attn.r_r_bias', 'transformer_layer.layer.8.ff.layer_1.bias', 'transformer_layer.layer.4.rel_attn.q', 'transformer_layer.layer.8.rel_attn.r_w_bias', 'transformer_layer.layer.0.rel_attn.r', 'transformer_layer.layer.11.rel_attn.q', 'transformer_layer.layer.7.rel_attn.r_w_bias', 'transformer_layer.layer.0.rel_attn.v', 'transformer_layer.layer.8.rel_attn.layer_norm.weight', 'transformer_layer.layer.2.rel_attn.r', 'transformer_layer.layer.7.rel_attn.layer_norm.weight', 'transformer_layer.layer.11.rel_attn.seg_embed', 'transformer_layer.layer.8.rel_attn.r_s_bias', 'transformer_layer.layer.2.rel_attn.r_s_bias', 'transformer_layer.layer.10.rel_attn.r', 'transformer_layer.layer.10.rel_attn.k', 'transformer_layer.layer.7.rel_attn.o', 'transformer_layer.layer.5.rel_attn.layer_norm.weight', 'transformer_layer.layer.8.ff.layer_1.weight', 'transformer_layer.layer.4.ff.layer_2.bias', 'transformer_layer.layer.10.rel_attn.seg_embed', 'transformer_layer.layer.5.rel_attn.layer_norm.bias', 'transformer_layer.layer.11.rel_attn.r_s_bias', 'longformer.pooler.dense.weight', 'transformer_layer.layer.10.rel_attn.r_r_bias', 'transformer_layer.layer.1.rel_attn.layer_norm.weight', 'transformer_layer.layer.8.rel_attn.r', 'transformer_layer.layer.0.rel_attn.r_s_bias', 'transformer_layer.layer.11.rel_attn.layer_norm.bias', 'transformer_layer.mask_emb', 'transformer_layer.layer.3.ff.layer_norm.bias', 'transformer_layer.layer.6.rel_attn.o', 'transformer_layer.layer.8.rel_attn.q', 'transformer_layer.layer.3.rel_attn.k', 'longformer.pooler.dense.bias', 'transformer_layer.layer.6.rel_attn.layer_norm.bias', 'transformer_layer.layer.8.rel_attn.seg_embed', 'transformer_layer.layer.8.rel_attn.r_r_bias', 'transformer_layer.layer.4.ff.layer_norm.weight', 'transformer_layer.layer.9.rel_attn.r', 'transformer_layer.layer.2.ff.layer_1.bias', 'transformer_layer.layer.0.ff.layer_norm.bias', 'transformer_layer.layer.0.rel_attn.o', 'transformer_layer.layer.7.ff.layer_2.bias', 'transformer_layer.layer.1.rel_attn.r_w_bias', 'transformer_layer.layer.5.rel_attn.r_r_bias', 'transformer_layer.layer.6.ff.layer_1.bias', 'transformer_layer.layer.2.rel_attn.layer_norm.bias', 'transformer_layer.layer.8.rel_attn.v', 'transformer_layer.layer.5.rel_attn.r_w_bias', 'transformer_layer.layer.7.rel_attn.seg_embed', 'transformer_layer.layer.7.ff.layer_norm.weight', 'transformer_layer.layer.9.rel_attn.o', 'transformer_layer.layer.0.ff.layer_1.bias', 'transformer_layer.layer.9.rel_attn.v', 'transformer_layer.layer.0.rel_attn.layer_norm.weight', 'transformer_layer.layer.9.rel_attn.q', 'transformer_layer.layer.6.rel_attn.layer_norm.weight', 'transformer_layer.layer.0.rel_attn.q', 'transformer_layer.layer.5.ff.layer_1.weight', 'transformer_layer.layer.9.rel_attn.layer_norm.bias', 'transformer_layer.layer.1.rel_attn.r_s_bias', 'transformer_layer.layer.1.ff.layer_1.bias', 'transformer_layer.layer.1.rel_attn.seg_embed', 'classifier.out_proj.bias', 'transformer_layer.layer.3.ff.layer_1.bias', 'transformer_layer.layer.4.rel_attn.r_w_bias', 'transformer_layer.layer.7.rel_attn.v', 'transformer_layer.layer.4.rel_attn.layer_norm.bias', 'transformer_layer.layer.1.rel_attn.o', 'transformer_layer.layer.3.ff.layer_1.weight', 'transformer_layer.layer.9.ff.layer_norm.bias', 'classifier.dense.weight', 'transformer_layer.layer.4.rel_attn.r_r_bias', 'transformer_layer.layer.11.ff.layer_norm.weight', 'transformer_layer.layer.6.rel_attn.r_r_bias', 'transformer_layer.layer.6.rel_attn.r', 'transformer_layer.layer.9.rel_attn.k', 'transformer_layer.layer.1.rel_attn.k', 'transformer_layer.layer.8.rel_attn.o', 'transformer_layer.layer.4.rel_attn.seg_embed', 'transformer_layer.layer.3.rel_attn.v', 'transformer_layer.layer.2.ff.layer_norm.bias', 'transformer_layer.layer.3.ff.layer_2.bias', 'transformer_layer.layer.1.rel_attn.r', 'transformer_layer.layer.9.rel_attn.seg_embed', 'transformer_layer.layer.3.rel_attn.seg_embed', 'transformer_layer.layer.6.ff.layer_2.bias', 'transformer_layer.layer.9.rel_attn.r_w_bias', 'transformer_layer.layer.5.ff.layer_2.bias', 'transformer_layer.layer.11.ff.layer_norm.bias', 'transformer_layer.layer.6.rel_attn.k', 'transformer_layer.layer.8.ff.layer_2.bias', 'transformer_layer.layer.11.rel_attn.v', 'transformer_layer.layer.10.ff.layer_2.bias', 'transformer_layer.layer.5.rel_attn.k', 'transformer_layer.layer.0.rel_attn.layer_norm.bias', 'transformer_layer.layer.7.rel_attn.r', 'transformer_layer.layer.4.rel_attn.layer_norm.weight', 'transformer_layer.layer.2.ff.layer_norm.weight', 'transformer_layer.layer.10.rel_attn.layer_norm.bias', 'transformer_layer.layer.5.ff.layer_2.weight', 'transformer_layer.layer.10.ff.layer_1.bias', 'transformer_layer.layer.11.rel_attn.layer_norm.weight', 'transformer_layer.layer.8.ff.layer_norm.bias', 'transformer_layer.layer.2.ff.layer_1.weight', 'transformer_layer.layer.1.ff.layer_2.bias', 'transformer_layer.layer.2.rel_attn.q', 'transformer_layer.layer.7.rel_attn.k', 'transformer_layer.layer.2.rel_attn.r_r_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed698d04a0e46e7aa5bad1e5df3f452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import LongformerTokenizerFast, LongformerModel, LongformerConfig, Trainer, TrainingArguments, EvalPrediction, AutoTokenizer, AutoModel\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel, LongformerClassificationHead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# read the dataframe\n",
    "train = pd.read_csv('../data/mimic3/5/bkp//train_data_5_level_1.csv')\n",
    "train['Chunk1'] = train['Chunk1'].fillna('')\n",
    "train['Chunk2'] = train['Chunk2'].fillna('')\n",
    "train['Chunk3'] = train['Chunk3'].fillna('')\n",
    "train['Chunk4'] = train['Chunk4'].fillna('')\n",
    "train['Chunk5'] = train['Chunk5'].fillna('')\n",
    "train['Chunk6'] = train['Chunk6'].fillna('')\n",
    "train['Chunk7'] = train['Chunk7'].fillna('')\n",
    "train['Chunk8'] = train['Chunk8'].fillna('')\n",
    "train['Chunk9'] = train['Chunk9'].fillna('')\n",
    "train['Chunk10'] = train['Chunk10'].fillna('')\n",
    "\n",
    "# Concatenate the 'Chunk' columns into a single 'CombinedChunk' column\n",
    "train['CombinedChunk'] = train['Chunk1'] + train['Chunk2'] + train['Chunk3'] + train['Chunk4'] + train['Chunk5'] + train['Chunk6'] + train['Chunk7'] + train['Chunk8'] + train['Chunk9'] + train['Chunk10']\n",
    "train = train.drop(['Chunk1', 'Chunk2', 'Chunk3', 'Chunk4', 'Chunk5', 'Chunk6', 'Chunk7', 'Chunk8', 'Chunk9', 'Chunk10'], axis=1)\n",
    "train\n",
    "column_order = ['hadm_id', 'CombinedChunk', '38.93', '401.9', '414.01', '427.31', '428']\n",
    "\n",
    "# Reorder the columns in the DataFrame based on the specified order\n",
    "train = train[column_order]\n",
    "train['labels'] = train[train.columns[2:]].values.tolist()\n",
    "train = train.drop(['38.93', '401.9', '414.01', '427.31', '428'], axis=1)\n",
    "print(train)\n",
    "\n",
    "\n",
    "# Read the 'dev' DataFrame from a CSV file (replace '/path/to/dev_data.csv' with the actual path)\n",
    "dev = pd.read_csv('../data/mimic3/5/bkp/dev_data_5_level_1.csv')\n",
    "\n",
    "# Fill NaN values in 'Chunk' columns with empty strings\n",
    "dev['Chunk1'] = dev['Chunk1'].fillna('')\n",
    "dev['Chunk2'] = dev['Chunk2'].fillna('')\n",
    "dev['Chunk3'] = dev['Chunk3'].fillna('')\n",
    "dev['Chunk4'] = dev['Chunk4'].fillna('')\n",
    "dev['Chunk5'] = dev['Chunk5'].fillna('')\n",
    "dev['Chunk6'] = dev['Chunk6'].fillna('')\n",
    "dev['Chunk7'] = dev['Chunk7'].fillna('')\n",
    "dev['Chunk8'] = dev['Chunk8'].fillna('')\n",
    "dev['Chunk9'] = dev['Chunk9'].fillna('')\n",
    "dev['Chunk10'] = dev['Chunk10'].fillna('')\n",
    "\n",
    "# Concatenate the 'Chunk' columns into a single 'CombinedChunk' column\n",
    "dev['CombinedChunk'] = dev['Chunk1'] + dev['Chunk2'] + dev['Chunk3'] + dev['Chunk4'] + dev['Chunk5'] + dev['Chunk6'] + dev['Chunk7'] + dev['Chunk8'] + dev['Chunk9'] + dev['Chunk10']\n",
    "dev = dev.drop(['Chunk1', 'Chunk2', 'Chunk3', 'Chunk4', 'Chunk5', 'Chunk6', 'Chunk7', 'Chunk8', 'Chunk9', 'Chunk10'], axis=1)\n",
    "\n",
    "# Define the desired column order for 'dev' (similar to 'train' and 'test')\n",
    "column_order = ['hadm_id', 'CombinedChunk', '38.93', '401.9', '414.01', '427.31', '428']\n",
    "\n",
    "# Reorder the columns in the 'dev' DataFrame based on the specified order\n",
    "dev = dev[column_order]\n",
    "\n",
    "# Create a 'labels' column in 'dev' (if needed)\n",
    "dev['labels'] = dev[dev.columns[2:]].values.tolist()\n",
    "dev = dev.drop(['38.93', '401.9', '414.01', '427.31', '428'], axis=1)\n",
    "print(dev)\n",
    "# Now, you have performed the same operations on the 'dev' DataFrame as you did for 'train' and 'test'\n",
    "# Read the 'test' DataFrame from a CSV file (replace '/path/to/test_data.csv' with the actual path)\n",
    "test = pd.read_csv('../data/mimic3/5/bkp/test_data_5_level_1.csv')\n",
    "\n",
    "# Fill NaN values in 'Chunk' columns with empty strings\n",
    "test['Chunk1'] = test['Chunk1'].fillna('')\n",
    "test['Chunk2'] = test['Chunk2'].fillna('')\n",
    "test['Chunk3'] = test['Chunk3'].fillna('')\n",
    "test['Chunk4'] = test['Chunk4'].fillna('')\n",
    "test['Chunk5'] = test['Chunk5'].fillna('')\n",
    "test['Chunk6'] = test['Chunk6'].fillna('')\n",
    "test['Chunk7'] = test['Chunk7'].fillna('')\n",
    "test['Chunk8'] = test['Chunk8'].fillna('')\n",
    "test['Chunk9'] = test['Chunk9'].fillna('')\n",
    "test['Chunk10'] = test['Chunk10'].fillna('')\n",
    "\n",
    "# Concatenate the 'Chunk' columns into a single 'CombinedChunk' column\n",
    "test['CombinedChunk'] = test['Chunk1'] + test['Chunk2'] + test['Chunk3'] + test['Chunk4'] + test['Chunk5'] + test['Chunk6'] + test['Chunk7'] + test['Chunk8'] + test['Chunk9'] + test['Chunk10']\n",
    "test = test.drop(['Chunk1', 'Chunk2', 'Chunk3', 'Chunk4', 'Chunk5', 'Chunk6', 'Chunk7', 'Chunk8', 'Chunk9', 'Chunk10'], axis=1)\n",
    "\n",
    "# Define the desired column order for 'test' (similar to 'train')\n",
    "column_order = ['hadm_id', 'CombinedChunk', '38.93', '401.9', '414.01', '427.31', '428']\n",
    "\n",
    "# Reorder the columns in the 'test' DataFrame based on the specified order\n",
    "test = test[column_order]\n",
    "\n",
    "# Create a 'labels' column in 'test' (if needed)\n",
    "test['labels'] = test[test.columns[2:]].values.tolist()\n",
    "test = test.drop(['38.93', '401.9', '414.01', '427.31', '428'], axis=1)\n",
    "print(test)\n",
    "# Now, you have performed the same operations on the 'test' DataFrame as you did for 'train'\n",
    "\n",
    "\n",
    "class LongformerForMultiLabelICDClassification(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    We instantiate a class of LongFormer adapted for a multilabel classification task.\n",
    "    This instance takes the pooled output of the LongFormer based model and passes it through a classification head. We replace the traditional Cross Entropy loss with a BCE loss that generate probabilities for all the labels that we feed into the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LongformerForMultiLabelICDClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.transformer_layer = AutoModel.from_pretrained(\"pretrained/ClinicalplusXLNet/\")\n",
    "        self.longformer = LongformerModel(config)\n",
    "        self.classifier = LongformerClassificationHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None,\n",
    "                token_type_ids=None, position_ids=None, inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        # create global attention on sequence, and a global attention token on the `s` token\n",
    "        # the equivalent of the CLS token on BERT models. This is taken care of by HuggingFace\n",
    "        # on the LongformerForSequenceClassification class\n",
    "        if global_attention_mask is None:\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "        #transformer_output = self.transformer_layer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        # pass arguments to longformer model\n",
    "        #transformer_output = tuple(torch.tensor(tf.convert_to_tensor(hs)).detach().numpy() for hs in transformer_output.hidden_states)\n",
    "        outputs = self.longformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            global_attention_mask = global_attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids)\n",
    "\n",
    "        # if specified the model can return a dict where each key corresponds to the output of a\n",
    "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
    "        # which will have the shape (batch_size, sequence_length, hidden_size).\n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "\n",
    "        # pass the hidden states through the classifier to obtain thee logits\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            labels = labels.float()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                            labels.view(-1, self.num_labels))\n",
    "            #outputs = (loss,) + outputs\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class Data_Processing(object):\n",
    "    def __init__(self, tokenizer, id_column, text_column, label_column):\n",
    "\n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "\n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column\n",
    "\n",
    "        # define the id column and transform it to list\n",
    "        self.id_column = id_column.tolist()\n",
    "\n",
    "\n",
    "# iter method to get each element at the time and tokenize it using bert\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.text_column[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "        # encode the sequence and add padding\n",
    "        inputs = tokenizer.encode_plus(comment_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length= 3048,\n",
    "                                       padding = 'max_length',\n",
    "                                       return_attention_mask = True,\n",
    "                                       truncation = True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        labels_ = torch.tensor(self.label_column[index], dtype=torch.float)\n",
    "        id_ = self.id_column[index]\n",
    "        return {'input_ids':input_ids[0], 'attention_mask':attention_mask[0],\n",
    "                'labels':labels_, 'id_':id_}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_column)\n",
    "\n",
    "batch_size = 2\n",
    "# create a class to process the training and test data\n",
    "tokenizer = AutoTokenizer.from_pretrained('yikuan8/Clinical-Longformer',\n",
    "                                                    padding = 'max_length',\n",
    "                                                    truncation=False,\n",
    "                                                    max_length = 4096,\n",
    "                                                    padding_side=\"right\")\n",
    "training_data = Data_Processing(tokenizer,\n",
    "                                train['hadm_id'],\n",
    "                                train['CombinedChunk'],\n",
    "                                train['labels'])\n",
    "dev_data = Data_Processing(tokenizer,\n",
    "                             dev['hadm_id'],\n",
    "                             dev['CombinedChunk'],\n",
    "                             dev['labels'])\n",
    "test_data =  Data_Processing(tokenizer,\n",
    "                             test['hadm_id'],\n",
    "                             test['CombinedChunk'],\n",
    "                             test['labels'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "                    'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                 'val':len(test_data)\n",
    "                }\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LongformerForMultiLabelICDClassification.from_pretrained('yikuan8/Clinical-Longformer',\n",
    "                                                  gradient_checkpointing=False,\n",
    "                                                  attention_window = 512,\n",
    "                                                  num_labels = 5,\n",
    "                                                  return_dict=True)\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "def multi_label_metrics(predictions, labels):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_true = labels\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # define dictionary of metrics to return\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "# Use the aux EvalPrediction class to obtain prediction labels\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions,\n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds,\n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '../model/new',\n",
    "    num_train_epochs = 10,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 64,\n",
    "    per_device_eval_batch_size= 2,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    disable_tqdm = False,\n",
    "    load_best_model_at_end=False,\n",
    "    warmup_steps = 2000,\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    fp16 = False,\n",
    "    logging_dir='/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'longformer_multilabel_paper_trainer_3048_2e5',\n",
    "    report_to = 'none'\n",
    ")\n",
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=dev_data,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #data_collator = Data_Processing(),\n",
    "\n",
    ")\n",
    "device = 'cpu'\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_predictions = trainer.predict(test_dataset=test_data)\n",
    "print(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11675231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
